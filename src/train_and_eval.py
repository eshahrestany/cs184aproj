import csv
import os
import random
import pickle

import kagglehub
import numpy as np
import pandas as pd
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

from .biomarker_models import (
    create_biomarker_model,
    create_random_forest_model,
    create_svm_model,
)
from .compute_biomarkers import FEATURE_NAMES, _compute_biomarkers_for_image


# ============================================================
# Dataset backed by features.csv (all TRAINING data)
# ============================================================

class BiomarkerDataset(Dataset):
    """
    Dataset backed by features.csv generated by compute_biomarkers.py.

    Each row has:
        ID, label, FEATURE_NAMES...

    We load all rows; ALL rows are used for training.
    """

    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.features, self.labels, self.ids = self._load_all()

    def _load_all(self):
        feats = []
        labels = []
        ids = []

        with open(self.csv_path, "r", newline="") as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    img_id = int(row["ID"])
                    label = float(row["label"])
                    feature_vec = [float(row[name]) for name in FEATURE_NAMES]
                except Exception as e:
                    print(f"[WARN] Skipping row due to parse error: {e}")
                    continue

                feats.append(feature_vec)
                labels.append(label)
                ids.append(img_id)

        if not feats:
            raise RuntimeError(f"No valid rows found in {self.csv_path}")

        features = torch.tensor(np.array(feats, dtype=np.float32), dtype=torch.float32)
        labels_t = torch.tensor(np.array(labels, dtype=np.float32), dtype=torch.float32)

        print(
            f"[INFO] Loaded dataset from {self.csv_path}: "
            f"{features.shape[0]} samples, {features.shape[1]} features"
        )

        return features, labels_t, ids

    def __len__(self):
        return self.labels.shape[0]

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


# ============================================================
# NN training helpers
# ============================================================

def train_epoch(model, loader, opt, criterion, device, mean, std):
    model.train()
    total_loss, total_correct, total = 0.0, 0, 0

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        x = (x - mean) / std

        opt.zero_grad()
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        opt.step()

        total_loss += float(loss.item()) * x.size(0)
        preds = (torch.sigmoid(logits) >= 0.5).float()
        total_correct += int((preds == y).sum())
        total += x.size(0)

    return total_loss / total, total_correct / total


def eval_epoch(model, loader, criterion, device, mean, std):
    model.eval()
    total_loss, total_correct, total = 0.0, 0, 0

    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            x = (x - mean) / std

            logits = model(x)
            loss = criterion(logits, y)

            total_loss += float(loss.item()) * x.size(0)
            preds = (torch.sigmoid(logits) >= 0.5).float()
            total_correct += int((preds == y).sum())
            total += x.size(0)

    return total_loss / total, total_correct / total


def build_test_biomarker_set(csv_path: str = "test_features.csv"):
    """
    Build or load biomarker test set.

    If test_features.csv exists:
        → load from disk and return (X_test, y_test)

    Otherwise:
        → walk test/benign and test/malignant,
          compute biomarkers with _compute_biomarkers_for_image(),
          save CSV, and return arrays.
    """

    # --------------------------------------------------------
    # Check for existing CSV
    # --------------------------------------------------------
    if os.path.exists(csv_path):
        print(f"[TEST] Loading existing biomarker test set: {csv_path}")
        df = pd.read_csv(csv_path)
        X_test = df[FEATURE_NAMES].values.astype(np.float32)
        y_test = df["label"].values.astype(int)
        print(f"[TEST] Loaded {X_test.shape[0]} test samples.")
        return X_test, y_test

    # --------------------------------------------------------
    # Otherwise create CSV from raw test images
    # --------------------------------------------------------
    print("\n[TEST] test_features.csv not found — building test biomarker set...")

    dataset_root = kagglehub.dataset_download(
        "hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images"
    )
    data_root = os.path.join(dataset_root, "melanoma_cancer_dataset")
    test_root = os.path.join(data_root, "test")

    splits = [
        ("benign", 0),
        ("malignant", 1),
    ]

    rows = []

    for subdir, label in splits:
        dir_path = os.path.join(test_root, subdir)
        if not os.path.isdir(dir_path):
            raise RuntimeError(f"Test directory not found: {dir_path}")

        files = sorted(
            f for f in os.listdir(dir_path)
            if f.lower().endswith((".jpg", ".jpeg", ".png"))
        )

        print(f"[TEST] {subdir}: {len(files)} images")

        for fname in files:
            img_path = os.path.join(dir_path, fname)

            # Compute all biomarkers (returns dict with ID, label, and all features)
            row = _compute_biomarkers_for_image(img_path, label)

            # Order into a strict CSV row
            ordered_row = {
                "ID": row["ID"],
                "label": row["label"],
            }
            for name in FEATURE_NAMES:
                ordered_row[name] = float(row[name])

            rows.append(ordered_row)

    # Save CSV
    df = pd.DataFrame(rows)
    df.to_csv(csv_path, index=False)
    print(f"[TEST] Saved biomarker test set to {csv_path}")

    # Return arrays
    X_test = df[FEATURE_NAMES].values.astype(np.float32)
    y_test = df["label"].values.astype(int)
    print(f"[TEST] Built {X_test.shape[0]} test samples.")

    return X_test, y_test


# ============================================================
# Main
# ============================================================

def main():
    features_csv = "./features.csv"

    if not os.path.exists(features_csv):
        raise FileNotFoundError(
            f"features.csv not found at {features_csv}. "
            f"Run compute_biomarkers.py first to generate it."
        )

    batch = 64
    epochs = 20
    lr = 1e-3
    wd = 1e-4
    seed = 42

    torch.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Using device: {device}")

    # --------------------------------------------------------
    # 1) Load FULL training dataset from features.csv
    # --------------------------------------------------------
    full_ds = BiomarkerDataset(features_csv)
    N = len(full_ds)

    # All rows are training data
    train_loader = DataLoader(full_ds, batch_size=batch, shuffle=True)

    # Normalization stats from ALL training features
    train_feats = full_ds.features
    mean = train_feats.mean(dim=0).to(device)
    std = train_feats.std(dim=0).to(device)
    std[std == 0] = 1.0

    mean_np = mean.cpu().numpy()
    std_np = std.cpu().numpy()

    def normalize_np(x_np: np.ndarray) -> np.ndarray:
        return (x_np - mean_np) / std_np

    # Numpy features / labels for sklearn models
    X_train_np = full_ds.features.numpy()
    y_train_np = full_ds.labels.numpy().astype(int)

    X_train_norm = normalize_np(X_train_np)

    # ============================================================
    # 2) Neural Net model (train on ALL training data)
    # ============================================================

    input_dim = len(FEATURE_NAMES)
    nn_model = create_biomarker_model(
        input_dim=input_dim,
        hidden_dims=(64, 32),
        dropout=0.1,
    ).to(device)

    criterion = nn.BCEWithLogitsLoss()
    opt = torch.optim.Adam(nn_model.parameters(), lr=lr, weight_decay=wd)

    best_tr_acc = 0.0
    nn_ckpt_path = "./models/bm_nn.pt"

    print("\n[NN] Starting training on FULL training set...")
    for ep in range(1, epochs + 1):
        tr_loss, tr_acc = train_epoch(nn_model, train_loader, opt, criterion, device, mean, std)
        print(f"[NN] Epoch {ep:03d} | train loss {tr_loss:.4f}, acc {tr_acc:.4f}")

        if tr_acc > best_tr_acc:
            best_tr_acc = tr_acc
            torch.save(
                {
                    "model": nn_model.state_dict(),
                    "mean": mean.cpu(),
                    "std": std.cpu(),
                    "feature_names": FEATURE_NAMES,
                },
                nn_ckpt_path,
            )
            print(f"[NN] Saved new best model to {nn_ckpt_path} (train acc={tr_acc:.4f})")

    print(f"[NN] Done. Best train acc = {best_tr_acc:.4f}")

    # ============================================================
    # 3) Random Forest (train on ALL training data)
    # ============================================================

    print("\n[RF] Training Random Forest on FULL training set...")
    rf_model = create_random_forest_model()
    rf_model.fit(X_train_norm, y_train_np)

    rf_ckpt_path = "./models/bm_rf.pkl"
    with open(rf_ckpt_path, "wb") as f:
        pickle.dump(
            {
                "model": rf_model,
                "feature_names": FEATURE_NAMES,
                "mean": mean_np,
                "std": std_np,
            },
            f,
        )
    print(f"[RF] Saved Random Forest checkpoint to {rf_ckpt_path}")

    # ============================================================
    # 4) SVM (train on ALL training data)
    # ============================================================

    print("\n[SVM] Training SVM on FULL training set...")
    svm_model = create_svm_model()
    svm_model.fit(X_train_norm, y_train_np)

    svm_ckpt_path = "./models/bm_svm.pkl"
    with open(svm_ckpt_path, "wb") as f:
        pickle.dump(
            {
                "model": svm_model,
                "feature_names": FEATURE_NAMES,
                "mean": mean_np,
                "std": std_np,
            },
            f,
        )
    print(f"[SVM] Saved SVM checkpoint to {svm_ckpt_path}")

    # ============================================================
    # 5) Build TEST set biomarkers from raw images and evaluate
    # ============================================================

    X_test, y_test = build_test_biomarker_set()
    X_test_norm = normalize_np(X_test)

    # --- NN test ---
    nn_model.eval()
    with torch.no_grad():
        X_test_t = torch.from_numpy(X_test).to(device)
        X_test_t = (X_test_t - mean) / std
        logits_test = nn_model(X_test_t)
        nn_pred = (torch.sigmoid(logits_test) >= 0.5).long().cpu().numpy().reshape(-1)
    nn_acc = (nn_pred == y_test).mean()
    print(f"\n[TEST] Biomarker NN test acc = {nn_acc:.4f}")

    # --- RF test ---
    rf_pred = rf_model.predict(X_test_norm).astype(int)
    rf_acc = (rf_pred == y_test).mean()
    print(f"[TEST] Random Forest test acc = {rf_acc:.4f}")

    # --- SVM test ---
    svm_pred = svm_model.predict(X_test_norm).astype(int)
    svm_acc = (svm_pred == y_test).mean()
    print(f"[TEST] SVM test acc          = {svm_acc:.4f}")

    # ============================================================
    # 6) E-class ensemble (majority vote over NN, RF, SVM)
    # ============================================================

    votes = np.stack([nn_pred, rf_pred, svm_pred], axis=1)  # (N_test, 3)
    vote_sum = votes.sum(axis=1)  # 0..3
    e_pred = (vote_sum >= 2).astype(int)
    e_acc = (e_pred == y_test).mean()

    print(f"[TEST] Ensemble (NN + RF + SVM) test acc = {e_acc:.4f}")
    print("\n[INFO] Finished training on features.csv and evaluating on image test set.")


if __name__ == "__main__":
    main()
