# train_and_eval.py

import csv
import os
import random
import pickle

import kagglehub
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, Subset

from biomarker_models import (
    create_biomarker_model,
    create_random_forest_model,
    create_svm_model,
)
from compute_biomarkers import FEATURE_NAMES


# ============================================================
# Dataset backed by features.csv (all training data)
# ============================================================

class BiomarkerDataset(Dataset):
    """
    Dataset backed by features.csv generated by compute_biomarkers.py.

    Each row has:
        ID, label, FEATURE_NAMES...

    We load all rows; train/val splitting is done in code.
    """

    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.features, self.labels, self.ids = self._load_all()

    def _load_all(self):
        feats = []
        labels = []
        ids = []

        with open(self.csv_path, "r", newline="") as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    img_id = int(row["ID"])
                    label = float(row["label"])
                    feature_vec = [float(row[name]) for name in FEATURE_NAMES]
                except Exception as e:
                    print(f"[WARN] Skipping row due to parse error: {e}")
                    continue

                feats.append(feature_vec)
                labels.append(label)
                ids.append(img_id)

        if not feats:
            raise RuntimeError(f"No valid rows found in {self.csv_path}")

        features = torch.tensor(np.array(feats, dtype=np.float32), dtype=torch.float32)
        labels_t = torch.tensor(np.array(labels, dtype=np.float32), dtype=torch.float32)

        print(
            f"[INFO] Loaded dataset from {self.csv_path}: "
            f"{features.shape[0]} samples, {features.shape[1]} features"
        )

        return features, labels_t, ids

    def __len__(self):
        return self.labels.shape[0]

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


# ============================================================
# NN training helpers
# ============================================================

def train_epoch(model, loader, opt, criterion, device, mean, std):
    model.train()
    total_loss, total_correct, total = 0.0, 0, 0

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        x = (x - mean) / std

        opt.zero_grad()
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        opt.step()

        total_loss += float(loss.item()) * x.size(0)
        preds = (torch.sigmoid(logits) >= 0.5).float()
        total_correct += int((preds == y).sum())
        total += x.size(0)

    return total_loss / total, total_correct / total


def eval_epoch(model, loader, criterion, device, mean, std):
    model.eval()
    total_loss, total_correct, total = 0.0, 0, 0

    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            x = (x - mean) / std

            logits = model(x)
            loss = criterion(logits, y)

            total_loss += float(loss.item()) * x.size(0)
            preds = (torch.sigmoid(logits) >= 0.5).float()
            total_correct += int((preds == y).sum())
            total += x.size(0)

    return total_loss / total, total_correct / total


# ============================================================
# Main
# ============================================================

def main():
    # 1) Download dataset (for consistency / environment), though training uses CSV only
    base = kagglehub.dataset_download(
        "hasnainjaved/melanoma-skin-cancer-dataset-of-10000-images"
    )
    data_root = os.path.join(base, "melanoma_cancer_dataset", "train")
    print(f"[INFO] Kaggle train data root (not directly used here): {data_root}")

    features_csv = "./features.csv"

    if not os.path.exists(features_csv):
        raise FileNotFoundError(
            f"features.csv not found at {features_csv}. "
            f"Run compute_biomarkers.py first to generate it."
        )

    batch = 64
    epochs = 20
    lr = 1e-3
    wd = 1e-4
    val_split = 0.2
    seed = 42

    torch.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Using device: {device}")

    # 2) Load full dataset from features.csv
    full_ds = BiomarkerDataset(features_csv)
    N = len(full_ds)

    # 3) Train/val split
    indices = torch.randperm(N)
    split = int(N * (1.0 - val_split))
    train_idx = indices[:split]
    val_idx = indices[split:]

    train_subset = Subset(full_ds, train_idx.tolist())
    val_subset = Subset(full_ds, val_idx.tolist())

    # Compute normalization stats from train subset only (for NN + optional RF/SVM)
    train_feats = full_ds.features[train_idx]
    mean = train_feats.mean(dim=0).to(device)
    std = train_feats.std(dim=0).to(device)
    std[std == 0] = 1.0

    mean_np = mean.cpu().numpy()
    std_np = std.cpu().numpy()

    def normalize_np(x_np: np.ndarray) -> np.ndarray:
        return (x_np - mean_np) / std_np

    train_loader = DataLoader(train_subset, batch_size=batch, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=batch, shuffle=False)

    # Extract raw numpy features / labels for sklearn models
    train_X_np = full_ds.features[train_idx].numpy()
    val_X_np = full_ds.features[val_idx].numpy()
    train_y_np = full_ds.labels[train_idx].numpy().astype(int)
    val_y_np = full_ds.labels[val_idx].numpy().astype(int)

    # Optionally normalize for RF/SVM as well (can help a bit, especially for SVM)
    train_X_norm = normalize_np(train_X_np)
    val_X_norm = normalize_np(val_X_np)

    # ============================================================
    # 4) Neural Net model
    # ============================================================

    input_dim = len(FEATURE_NAMES)  # 21 with current setup
    nn_model = create_biomarker_model(
        input_dim=input_dim,
        hidden_dims=(64, 32),
        dropout=0.1,
    ).to(device)

    criterion = nn.BCEWithLogitsLoss()
    opt = torch.optim.Adam(nn_model.parameters(), lr=lr, weight_decay=wd)

    best_nn_acc = 0.0
    nn_ckpt_path = "models/bm_nn.pt"

    print("\n[NN] Starting training...")
    for ep in range(1, epochs + 1):
        tr_loss, tr_acc = train_epoch(nn_model, train_loader, opt, criterion, device, mean, std)
        va_loss, va_acc = eval_epoch(nn_model, val_loader, criterion, device, mean, std)

        print(
            f"[NN] Epoch {ep:03d} | "
            f"train loss {tr_loss:.4f}, acc {tr_acc:.4f} | "
            f"val loss {va_loss:.4f}, acc {va_acc:.4f}"
        )

        if va_acc > best_nn_acc:
            best_nn_acc = va_acc
            torch.save(
                {
                    "model": nn_model.state_dict(),
                    "mean": mean.cpu(),
                    "std": std.cpu(),
                    "feature_names": FEATURE_NAMES,
                },
                nn_ckpt_path,
            )
            print(f"[NN] Saved new best model to {nn_ckpt_path} (val acc={va_acc:.4f})")

    print(f"[NN] Done. Best val acc = {best_nn_acc:.4f}")

    # ============================================================
    # 5) Random Forest
    # ============================================================

    print("\n[RF] Training Random Forest...")
    rf_model = create_random_forest_model()
    rf_model.fit(train_X_norm, train_y_np)
    rf_val_pred = rf_model.predict(val_X_norm)
    rf_val_acc = (rf_val_pred == val_y_np).mean()
    print(f"[RF] Val acc = {rf_val_acc:.4f}")

    rf_ckpt_path = "models/bm_rf.pkl"
    with open(rf_ckpt_path, "wb") as f:
        pickle.dump(
            {
                "model": rf_model,
                "feature_names": FEATURE_NAMES,
                "mean": mean_np,
                "std": std_np,
            },
            f,
        )
    print(f"[RF] Saved Random Forest checkpoint to {rf_ckpt_path}")

    # ============================================================
    # 6) SVM
    # ============================================================

    print("\n[SVM] Training SVM...")
    svm_model = create_svm_model()
    svm_model.fit(train_X_norm, train_y_np)
    svm_val_pred = svm_model.predict(val_X_norm)
    svm_val_acc = (svm_val_pred == val_y_np).mean()
    print(f"[SVM] Val acc = {svm_val_acc:.4f}")

    svm_ckpt_path = "models/bm_svm.pkl"
    with open(svm_ckpt_path, "wb") as f:
        pickle.dump(
            {
                "model": svm_model,
                "feature_names": FEATURE_NAMES,
                "mean": mean_np,
                "std": std_np,
            },
            f,
        )
    print(f"[SVM] Saved SVM checkpoint to {svm_ckpt_path}")

    # ============================================================
    # 7) E-class ensemble: majority vote over NN, RF, SVM
    # ============================================================

    print("\n[E] Evaluating E-class ensemble (majority vote)...")

    # NN predictions on val set
    nn_model.eval()
    val_logits_all = []
    with torch.no_grad():
        for x, _ in val_loader:
            x = x.to(device)
            x_norm = (x - mean) / std
            logits = nn_model(x_norm)
            val_logits_all.append(logits.cpu())
    val_logits_all = torch.cat(val_logits_all, dim=0)
    nn_val_pred = (torch.sigmoid(val_logits_all) >= 0.5).long().numpy()

    # RF / SVM predictions already computed
    rf_val_pred = rf_val_pred.astype(int)
    svm_val_pred = svm_val_pred.astype(int)

    # sanity: make sure lengths line up with val_y_np
    assert nn_val_pred.shape[0] == val_y_np.shape[0], "NN val size mismatch"
    assert rf_val_pred.shape[0] == val_y_np.shape[0], "RF val size mismatch"
    assert svm_val_pred.shape[0] == val_y_np.shape[0], "SVM val size mismatch"

    votes = np.stack([nn_val_pred, rf_val_pred, svm_val_pred], axis=1)  # (N_val, 3)
    vote_sum = votes.sum(axis=1)  # 0..3
    e_val_pred = (vote_sum >= 2).astype(int)

    e_val_acc = (e_val_pred == val_y_np).mean()
    print(f"[E] Ensemble (NN + RF + SVM) val acc = {e_val_acc:.4f}")

    print("\n[INFO] Finished training/evaluation for NN, RF, SVM, and E-class ensemble.")


if __name__ == "__main__":
    main()
